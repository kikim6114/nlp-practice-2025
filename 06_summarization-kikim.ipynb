{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cPdJ5xFwjcUg"
   },
   "source": [
    "<table align=\"left\"><tr><td>\n",
    "<a href=\"https://colab.research.google.com/github/kikim6114/NLP2024-1/blob/main/06_summarization-kikim.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"ì½”ë©ì—ì„œ ì‹¤í–‰í•˜ê¸°\"/></a>\n",
    "</td></tr></table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W8-NJsjfCZMG"
   },
   "source": [
    "ì´ ë…¸íŠ¸ë¶ì„ ì½”ë©ì—ì„œ ì‹¤í–‰í•˜ë ¤ë©´ Pro ë²„ì „ì´ í•„ìš”í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 87477,
     "status": "ok",
     "timestamp": 1747774731066,
     "user": {
      "displayName": "ê¹€ê´‘ì¼",
      "userId": "10470365924443187526"
     },
     "user_tz": -540
    },
    "id": "AGC8Bp4OJ4bw",
    "outputId": "c847f87d-7538-4df1-bbcc-237dd8123c9b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'nlp-practice-2025'...\n",
      "remote: Enumerating objects: 205, done.\u001b[K\n",
      "remote: Counting objects: 100% (15/15), done.\u001b[K\n",
      "remote: Compressing objects: 100% (5/5), done.\u001b[K\n",
      "remote: Total 205 (delta 11), reused 11 (delta 10), pack-reused 190 (from 1)\u001b[K\n",
      "Receiving objects: 100% (205/205), 29.30 MiB | 16.28 MiB/s, done.\n",
      "Resolving deltas: 100% (52/52), done.\n",
      "/content/nlp-practice-2025\n",
      "â³ Installing base requirements ...\n",
      "âœ… Base requirements installed!\n",
      "Using transformers v4.51.3\n",
      "Using datasets v2.14.4\n",
      "Using accelerate v1.6.0\n",
      "Using sentencepiece v0.2.0\n",
      "Using sacrebleu v2.5.1\n",
      "Using rouge_score\n",
      "Using nltk v3.9.1\n",
      "Using py7zr v0.22.0\n"
     ]
    }
   ],
   "source": [
    "# ì½”ë©ì„ ì‚¬ìš©í•˜ì§€ ì•Šìœ¼ë©´ ë‹¤ìŒ ì½”ë“œë¥¼ ì£¼ì„ ì²˜ë¦¬í•˜ì„¸ìš”.\n",
    "# !git clone https://github.com/kikim6114/nlp-practice-2025.git\n",
    "# %cd nlp-practice-2025\n",
    "# from install import *\n",
    "# install_requirements(chapter=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "executionInfo": {
     "elapsed": 5076,
     "status": "ok",
     "timestamp": 1747774761466,
     "user": {
      "displayName": "ê¹€ê´‘ì¼",
      "userId": "10470365924443187526"
     },
     "user_tz": -540
    },
    "id": "01aJFMv0jcUl"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\kikim\\anaconda3\\envs\\nlp\\lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline, set_seed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0zqROZkPjcUl"
   },
   "source": [
    "# 5. ìš”ì•½"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CTl_vP53eu7z"
   },
   "source": [
    "ìš”ì•½(Summarization)ì´ ë„ì „ì  ì‘ì—…ì¸ ì´ìœ :\n",
    "- ê¸´ ë‹¨ë½ì˜ ì´í•´\n",
    "- ê´€ë ¨ ë‚´ìš©ì˜ ì¶”ë¡ \n",
    "- ì›ë˜ ë¬¸ì„œì˜ ì£¼ì œë¥¼ í†µí•©í•´ ìœ ì°½í•œ í…ìŠ¤íŠ¸ë¥¼ ìƒì„±\n",
    "- ì˜ì—­ë³„ ìš”ì•½ ë°©ë²•ì´ ë‹¤ë¥´ë‹¤(ì˜ˆ: ê¸°ì‚¬ì™€ ë²•ë¥ ê³„ì•½ì„œì˜ ìš”ì•½ ë°©ë²•ì€ ë‹¤ë¥´ë‹¤)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8h0GZCptjcUl"
   },
   "source": [
    "## 5.1 CNN/DailyMail ë°ì´í„°ì…‹"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dyeqxYiqeu70"
   },
   "source": [
    "CNN/DailyMail dataset\n",
    "- 300,000ê°œ ë‰´ìŠ¤ ê¸°ì‚¬ì™€ ìš”ì•½ì˜ ìŒ\n",
    "- ìš”ì•½ì„ ìœ„í•´ ìµëª…í™” ì²˜ë¦¬ë˜ì§€ ì•Šì€ v3.0.0 ì‚¬ìš©\n",
    "- ìš”ì•½: ë³¸ë¬¸ì—ì„œ ì¶”ì¶œ(extractive)í•˜ì§€ ì•Šê³  ì¶”ìƒì (abstractive)ì´ë‹¤.\n",
    "  - Extractive: ë³¸ë¬¸ì˜ ì–´ë–¤ êµ¬ê°„ì„ ê°€ì ¸ë‹¤ ì‚¬ìš©í•¨\n",
    "  - Abstractive: ë‚´ìš©ì— ë§ì¶° ê¸€ì„ ìƒì„±"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -U datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 403,
     "referenced_widgets": [
      "33b2d082615143ca9c40b5b6a98e4fc8",
      "d834ca2ee2ff4cc9a80382b216caaa79",
      "ba12f39c667b4461901d191df39f4a20",
      "9ed594484a5040f38b04d95fbc8448ea",
      "114118bc4d60432eb6e5920c852c4516",
      "aebe566de65c4feba8f808129e98aa08",
      "bad8a6711b7b4290849616bbf5845888",
      "0088de2f78bb4a3aaf64536df94ff2af",
      "21218838820e479b92c8f3dbec98f7b4",
      "17cdf4fcffa243d3a3038aa4f7562562",
      "3f03d7b3f0ab448199f7e31986f98fe7"
     ]
    },
    "executionInfo": {
     "elapsed": 6337,
     "status": "error",
     "timestamp": 1747775389834,
     "user": {
      "displayName": "ê¹€ê´‘ì¼",
      "userId": "10470365924443187526"
     },
     "user_tz": -540
    },
    "id": "KfM-0Y3qjcUl",
    "outputId": "e9889cb2-cab6-42e2-efd7-f576a921683d"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a09361acf2954941aa376dfc47ab72fd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "cnn_stories.tgz:   0%|          | 0.00/159M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "82537ebebd004a85add3d50e10a30795",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "dailymail_stories.tgz:   0%|          | 0.00/376M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df84462f687f42cea3dc51499a2dd1f4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/2.11M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6cd0d43fe26747dda3e2fc8a78ac2f85",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/46.4M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bfbcb2e2b1b94e60b3256be563927e31",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/2.43M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "540a0414ed5e4293a1444e61312eeef4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cea289d1d5ab48b6b6e3afdaa737987b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "be813a67985f4f01b76f2b448ef2fa7d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "íŠ¹ì„±: ['article', 'highlights', 'id']\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset, config\n",
    "# !rm -rf {config.HF_DATASETS_CACHE}\n",
    "dataset = load_dataset(\"ccdv/cnn_dailymail\", version=\"3.0.0\", trust_remote_code=True)\n",
    "# dataset = load_dataset(\"ccdv/cnn_dailymail\", version=\"3.0.0\")  # original\n",
    "print(f\"íŠ¹ì„±: {dataset['train'].column_names}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "ZRQmpqZbjcUm"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ê¸°ì‚¬ (500ê°œ ë¬¸ì ë°œì·Œ, ì´ ê¸¸ì´: 3192):\n",
      "(CNN) -- Usain Bolt rounded off the world championships Sunday by claiming his third gold in Moscow as he anchored Jamaica to victory in the men's 4x100m relay. The fastest man in the world charged clear of United States rival Justin Gatlin as the Jamaican quartet of Nesta Carter, Kemar Bailey-Cole, Nickel Ashmeade and Bolt won in 37.36 seconds. The U.S finished second in 37.56 seconds with Canada taking the bronze after Britain were disqualified for a faulty handover. The 26-year-old Bolt has n\n",
      "\n",
      "ìš”ì•½ (ê¸¸ì´: 180):\n",
      "Usain Bolt wins third gold of world championship .\n",
      "Anchors Jamaica to 4x100m relay victory .\n",
      "Eighth gold at the championships for Bolt .\n",
      "Jamaica double up in women's 4x100m relay .\n"
     ]
    }
   ],
   "source": [
    "sample = dataset[\"train\"][1]\n",
    "print(f\"\"\"ê¸°ì‚¬ (500ê°œ ë¬¸ì ë°œì·Œ, ì´ ê¸¸ì´: {len(sample[\"article\"])}):\"\"\")\n",
    "print(sample[\"article\"][:500])\n",
    "print(f'\\nìš”ì•½ (ê¸¸ì´: {len(sample[\"highlights\"])}):')\n",
    "print(sample[\"highlights\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7bazY7Yqeu70"
   },
   "source": [
    "ê¸°ì‚¬ê°€ ê¸´ ê²½ìš°, ì…ë ¥ ì‹œí€€ìŠ¤ ìµœëŒ€ ê¸¸ì´ê°€ 1024ì¸ Transformer íŠ¹ì„±ìƒ ë’·ë¶€ë¶„ì˜ ê¸°ì‚¬ì— í¬í•¨ëœ ì •ë³´ëŠ” ì‚¬ë¼ì§„ë‹¤."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NpvI3GeJjcUm"
   },
   "source": [
    "## 6.2 í…ìŠ¤íŠ¸ ìš”ì•½ íŒŒì´í”„ë¼ì¸\n",
    "- ì—¬ëŸ¬ íŠ¸ëœìŠ¤í¬ë¨¸ ê¸°ë°˜ ìš”ì•½ ëª¨ë¸ì„ ì‚´í´ë³´ì.\n",
    "- ë¹„êµë¥¼ ìœ„í•´ ì…ë ¥ì˜ ê¸¸ì´ê°€ ëª¨ë‘ ë™ì¼í•˜ë„ë¡ ê¸¸ì´ë¥¼ 2000 ê¸€ìë¡œ ì œí•œí•œë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "5qqDExG9jcUn"
   },
   "outputs": [],
   "source": [
    "sample_text = dataset[\"train\"][1][\"article\"][:2000]\n",
    "# ë”•ì…”ë„ˆë¦¬ì— ê° ëª¨ë¸ì´ ìƒì„±í•œ ìš”ì•½ì„ ì €ì¥í•©ë‹ˆë‹¤.\n",
    "summaries = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YHxXimlUeu71"
   },
   "source": [
    "- í…ìŠ¤íŠ¸ë¥¼ ë¬¸ì¥ ë‹¨ìœ„ë¡œ ë¶„ë¦¬í•˜ëŠ” ê²ƒì´ ê´€í–‰ì´ë¯€ë¡œ,\n",
    "- ì¢‹ì€ ì„±ëŠ¥ì„ ê°–ëŠ” NLTK íŒ¨í‚¤ì§€ë¥¼ ì‚¬ìš©í•˜ì—¬ ë¶„í• í•œë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "xacCeKxVjcUn"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\kikim\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\kikim\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt_tab.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "nltk.download(\"punkt\")\n",
    "nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "hTSruLbcjcUo"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The U.S. are a country.', 'The U.N. is an organization.']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "string = \"The U.S. are a country. The U.N. is an organization.\"\n",
    "sent_tokenize(string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hMh2QRGnjcUo"
   },
   "source": [
    "### 6.2.1 ìš”ì•½ ê¸°ì¤€ ëª¨ë¸"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xYkA5SDfeu71"
   },
   "source": [
    "- ìš”ì•½ëœ í…ìŠ¤íŠ¸ì˜ ì²˜ìŒ 3 ë¬¸ì¥ë§Œ ê°€ì ¸ì˜¤ëŠ” í•¨ìˆ˜ë¥¼ ì •ì˜\n",
    "- ë¬¸ì¥ êµ¬ë¶„ì€ `\\n` ë¬¸ì"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "nAyE6oUHjcUo"
   },
   "outputs": [],
   "source": [
    "def three_sentence_summary(text):\n",
    "    return \"\\n\".join(sent_tokenize(text)[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "IAriHIE5jcUo"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'baseline': \"(CNN) -- Usain Bolt rounded off the world championships Sunday by claiming his third gold in Moscow as he anchored Jamaica to victory in the men's 4x100m relay.\\nThe fastest man in the world charged clear of United States rival Justin Gatlin as the Jamaican quartet of Nesta Carter, Kemar Bailey-Cole, Nickel Ashmeade and Bolt won in 37.36 seconds.\\nThe U.S finished second in 37.56 seconds with Canada taking the bronze after Britain were disqualified for a faulty handover.\"}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summaries[\"baseline\"] = three_sentence_summary(sample_text)\n",
    "summaries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tnHpv24zjcUp"
   },
   "source": [
    "### 6.2.2 GPT-2\n",
    "- ì…ë ¥ í…ìŠ¤íŠ¸ ëì— `TL;DR`ì„ ë¶™ì—¬ì„œ ìš”ì•½ì„ ìƒì„±í•˜ê²Œ í•  ìˆ˜ ìˆë‹¤.\n",
    "- `TL;DR`: Too Long; Didn't Read (Reddit ê°™ì€ ì‚¬ì´íŠ¸ì—ì„œ ê¸´ ê¸€ì„ ì¤‘ê°„ì— ìƒëµí•˜ëŠ” ë°©ë²•ìœ¼ë¡œ ì‚¬ìš©)\n",
    "- sample_text + \"\\nTL;DR:\\n\" ì„ ì…ë ¥í•˜ì—¬ ğŸ¤—Transformersì˜ `pipeline()`ìœ¼ë¡œ ì›ë˜ ê¸°ì‚¬ë¥¼ ì¬ìƒì„±í•˜ëŠ” ê²ƒìœ¼ë¡œ ì‹¤í—˜ì„ ì‹œì‘í•œë‹¤ëŠ” ê²ƒì— ì£¼ëª©í•˜ì."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "yg7tBbz5jcUp"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c4bbf4364a3a4567af442abed2aad0bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/666 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kikim\\anaconda3\\envs\\nlp\\lib\\site-packages\\huggingface_hub\\file_download.py:142: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\kikim\\.cache\\huggingface\\hub\\models--gpt2-large. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "597c2606cefb4fce996a74a26497fca4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/3.25G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a3e32a264cfb49e09994dd46244e2a1c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bdb2aadfe3f74cc18bc3bd95d5e7e08c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "23b6d08ae50943669e23272469c6f5d3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa33c17a165c4ae9b8e6ef441b8b704d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8b64b6e7844f40beaa992f013e9bcbcd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline, set_seed\n",
    "\n",
    "set_seed(42)\n",
    "\n",
    "# ì½”ë©ì˜ ê²½ìš° gpt2-xlì„ ì‚¬ìš©í•˜ë©´ ë©”ëª¨ë¦¬ ë¶€ì¡± ì—ëŸ¬ê°€ ë°œìƒí•©ë‹ˆë‹¤.\n",
    "# ëŒ€ì‹  \"gpt\" ë˜ëŠ” \"gpt2-large\"ë¡œ ì§€ì •í•˜ê±°ë‚˜ ì½”ë© í”„ë¡œë¥¼ ì‚¬ìš©í•˜ì„¸ìš”.\n",
    "pipe = pipeline(\"text-generation\", model=\"gpt2-large\")\n",
    "\n",
    "gpt2_query = sample_text + \"\\nTL;DR:\\n\"  # í”„ë¡¬í”„íŠ¸ í˜•ì‹ì„ ë§Œë“ ë‹¤.\n",
    "pipe_out = pipe(gpt2_query, max_length=512, clean_up_tokenization_spaces=True)\n",
    "summaries[\"gpt2\"] = \"\\n\".join(\n",
    "    sent_tokenize(pipe_out[0][\"generated_text\"][len(gpt2_query) :]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "79xsgPT9jcUp"
   },
   "source": [
    "### 6.2.3 T5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cb3xKSwIjcUp"
   },
   "source": [
    "<img alt=\"T5\" width=\"700\" caption=\"Diagram of T5's text-to-text framework (courtesy of Colin Raffel); besides translation and summarization, the CoLA (linguistic acceptability) and STSB (semantic similarity) tasks are shown\" src=\"images/chapter06_T5.png\" id=\"T5\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GZWzKYFkeu72"
   },
   "source": [
    "- T5: ëª¨ë“  NLP taskë“¤ì„ text-to-tex ì‘ì—…ìœ¼ë¡œ êµ¬ì„±í•˜ëŠ” universal transformer ì•„í‚¤í…ì²˜\n",
    "- T5ëŠ” ìš”ì•½ì„ í¬í•¨í•˜ëŠ” ì§€ë„ ë° ë¹„ì§€ë„ í•™ìŠµë°ì´í„°ë¡œ í›ˆë ¨ë¨\n",
    "- Fine-tuning ì—†ì´ pretrainingì— ì‚¬ìš©í–ˆë˜ í”„ë¡¬í”„íŠ¸ë¥¼ ì‚¬ìš©í•´ ë°”ë¡œ ìš”ì•½, ë²ˆì—­ ë“±ì˜ ì‘ì—…ì— ì‚¬ìš©í•  ìˆ˜ ìˆë‹¤.\n",
    "  - ìš”ì•½ format: \"summarize: \\<ARTICLE\\>\"\n",
    "  - ë²ˆì—­ format: \"translate English to German: \\<TEXT\\>\"\n",
    "- í•¨ìˆ˜ `pipeline(\"summarization\", model=\"t5-large\")`ëŠ” ì…ë ¥ì„ text-to-text formatìœ¼ë¡œ ì²˜ë¦¬í•˜ë¯€ë¡œ \"summarize\"ë¥¼ ë¶™ì¼ í•„ìš”ê°€ ì—†ë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KozBMCtPjcUp"
   },
   "outputs": [],
   "source": [
    "pipe = pipeline(\"summarization\", model=\"t5-large\")\n",
    "pipe_out = pipe(sample_text)\n",
    "summaries[\"t5\"] = \"\\n\".join(sent_tokenize(pipe_out[0][\"summary_text\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cKRgrztojcUq"
   },
   "source": [
    "### 6.2.4 BART"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WPkY4dEVeu72"
   },
   "source": [
    "- íŠ¸ëœìŠ¤í¬ë¨¸ë¥¼ ê¸°ë°˜ìœ¼ë¡œí•œ seq2seq ëª¨ë¸(encoder-decoder)ë¡œ êµ¬ì¶•ëœ denoising auto encoder\n",
    "- BERTì™€ GPT-2 ì‚¬ì „í›ˆë ¨ ë°©ì‹ì„ ê²°í•©í•˜ì—¬ í›ˆë ¨\n",
    "- ì‚¬ì „í•™ìŠµ ì¤‘ì— BERT ë¥˜ì˜ ì´ì „ ëª¨ë¸ë“¤ ë³´ë‹¤ ë” ê´‘ë²”ìœ„í•œ ë…¸ì´ì¦ˆ ì²´ê³„ë¥¼ ì œê³µ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o8kE833Yeu72"
   },
   "source": [
    "<img alt=\"T5\" width=\"500\" src=\"images/chapter06_bart.png\" id=\"bart\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T8UcU5Oeeu72"
   },
   "source": [
    "- ì—¬ê¸°ì„œëŠ” `CNN/DailyMail`ì— ë¯¸ì„¸ì¡°ì •ëœ `facebook/bart-large-ccn` checkpointë¥¼ ì‚¬ìš©"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dKPydAdujcUq"
   },
   "outputs": [],
   "source": [
    "pipe = pipeline(\"summarization\", model=\"facebook/bart-large-cnn\")\n",
    "pipe_out = pipe(sample_text)\n",
    "summaries[\"bart\"] = \"\\n\".join(sent_tokenize(pipe_out[0][\"summary_text\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4cVEAi9JjcUq"
   },
   "source": [
    "### 6.2.5 PEGASUS\n",
    "- Zhang, Jingqing, et al. \"Pegasus: Pre-training with extracted gap-sentences for abstractive summarization.\" International conference on machine learning. PMLR, 2020."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HgHBlHFbjcUq"
   },
   "source": [
    "<img alt=\"pegasus\" width=\"700\" caption=\"Diagram of PEGASUS architecture (courtesy of Jingqing Zhang et al.)\" src=\"images/chapter06_pegasus.png\" id=\"pegasus\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mqpK8mzaeu72"
   },
   "source": [
    "- ë…¼ë¬¸ ì œëª©ì—ì„œ ì•Œ ìˆ˜ ìˆë“¯ì´ Abstractive Summarizationì— íŠ¹í™”ëœ ëª¨ë¸\n",
    "- BARTì™€ ë§ˆì°¬ê°€ì§€ë¡œ Encoder-Decoder íŠ¸ëœìŠ¤í¬ë¨¸\n",
    "- Token ë‹¨ìœ„ê°€ ì•„ë‹Œ `Important Sentence` ë‹¨ìœ„ë¡œ masking í•˜ê³  ì´ë¥¼ ìƒì„±í•˜ëŠ” ë°©ë²•(Gap Sentence Generation; GSG)ìœ¼ë¡œ í•™ìŠµ.\n",
    "- Important sentence: Document ë‚´ì—ì„œ ë‹¤ë¥¸ ë¬¸ì¥ì— ë¹„í•´ ì „ì²´ì ì¸ contextë¥¼ ì˜ ì„¤ëª…í•  ìˆ˜ ìˆëŠ” ë¬¸ì¥."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "n7fCGRTbjcUq"
   },
   "outputs": [],
   "source": [
    "pipe = pipeline(\"summarization\", model=\"google/pegasus-cnn_dailymail\")\n",
    "pipe_out = pipe(sample_text)\n",
    "summaries[\"pegasus\"] = pipe_out[0][\"summary_text\"].replace(\" .<n>\", \".\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0yxm9cIReu73"
   },
   "source": [
    "- ì´ ëª¨ë¸ì€ ìë™ ì¤„ë°”ê¿ˆí•˜ëŠ” íŠ¹ìˆ˜ í† í°ì´ ìˆì–´ `sent_tokenize()` í•¨ìˆ˜ê°€ í•„ìš” ì—†ìŒ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tWN7GnDojcUq"
   },
   "source": [
    "## 6.3 ìš”ì•½ ê²°ê³¼ ë¹„êµí•˜ê¸°"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CGEZJIlPeu73"
   },
   "source": [
    "- GPT-2ëŠ” ë°ì´í„°ì…‹ì—ì„œ ì „í˜€ í›ˆë ¨ë˜ì§€ ì•Šì•˜ìŒì„ ì£¼ëª©.\n",
    "- T5ëŠ” ì—¬ëŸ¬ ì‘ì—… ì¤‘ì˜ í•˜ë‚˜ë¡œ ì´ ì‘ì—…ì„ ìœ„í•´ ë¯¸ì„¸ì¡°ì •ë¨.\n",
    "- BARTì™€ PEGASUSëŠ” ì´ ì‘ì—…ë§Œì„ ìœ„í•´ ë¯¸ì„¸ì¡°ì •ë¨."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "S7IrtSUvjcUq",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"GROUND TRUTH\")\n",
    "print(dataset[\"train\"][1][\"highlights\"])\n",
    "print(\"\")\n",
    "\n",
    "for model_name in summaries:\n",
    "    print(model_name.upper())\n",
    "    print(summaries[model_name])\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tPhl2itneu76"
   },
   "source": [
    "- GPT-2: **Hallucination**, **invented facts**\n",
    "- ë‚˜ë¨¸ì§€ 3ê°œ ëª¨ë¸ì˜ ìš”ì•½ì„ ì •ë‹µê³¼ ë¹„êµí•˜ë©´ ë†€ë„ ì •ë„ë¡œ ë§ì´ ì¤‘ë³µë˜ë©°, ê·¸ì¤‘ì—ì„œ PEGASUSê°€ ì •ë‹µì— ê°€ì¥ ê°€ê¹ë‹¤."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j9NOUMJKeu76"
   },
   "source": [
    "ì‹¤ ì‚¬ìš© í™˜ê²½ì— ì‚¬ìš©í•  ëª¨ë¸ì˜ ì„ íƒ:\n",
    "- ëª‡ ê°œì˜ ìƒ˜í”Œì„ ì¶”ê°€ë¡œ ìš”ì•½í•´ ë³´ëŠ” ê²ƒì€ ìµœì„ ì˜ ëª¨ë¸ì„ ì„ íƒí•˜ëŠ”ì€ ì²´ê³„ì  ë°©ë²•ì´ ì•„ë‹ˆë‹¤.\n",
    "- ì§€í‘œ í•˜ë‚˜ë¥¼ ì •ì˜í•˜ê³  ì–´ë–¤ ë²¤ì¹˜ë§ˆí¬ ë°ì´í„°ì…‹ì—ì„œ ëª¨ë“  ëª¨ë¸ì„ í‰ê°€í•´ì„œ ì„±ëŠ¥ì´ ìµœê³ ì¸ ëª¨ë¸ì„ ì„ íƒí•˜ëŠ” ê²ƒì´ ì´ìƒì ì´ ë°©ë²•ì´ë‹¤.\n",
    "- ì–´ë–¤ ì§€í‘œë¥¼ ì •ì˜í•´ì•¼ í…ìŠ¤íŠ¸ ìƒì„±ì— ìš°ìˆ˜í•œ ëª¨ë¸ì„ íŒë³„í•  ìˆ˜ ìˆì„ê¹Œ?\n",
    "  - accuracy, recall, ê·¸ë¦¬ê³  precision ê°™ì€ ì§€í‘œëŠ” ì´ ì‘ì—…ì— ì ìš©í•˜ê¸° ì–´ë µë‹¤\n",
    "  - ì‚¬ëŒì´ ì‘ì„±í•œ 'Gold standard' ìš”ì•½ë§ˆë‹¤ ë™ì˜ì–´, ë‹¤ë¥¸ ë§ë¡œ ë°”ê¿” ì“°ê±°ë‚˜, ë˜ëŠ” ì¡°ê¸ˆ ë‹¤ë¥´ê²Œ ì‘ì„±í•˜ëŠ” ë°©ì‹ìœ¼ë¡œ ìˆ˜ì‹­ê°œì˜ ìš”ì•½ì´ ê°€ëŠ¥í•˜ê¸° ë•Œë¬¸."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hBzK4BzmjcUr"
   },
   "source": [
    "## 6.4 ìƒì„±ëœ í…ìŠ¤íŠ¸ í’ˆì§ˆ í‰ê°€í•˜ê¸°"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nqYiXX08eu76"
   },
   "source": [
    "- í…ìŠ¤íŠ¸ ìƒì„± ì‘ì—…ì˜ í‰ê°€ëŠ” í‘œì¤€ì ì¸ ë¶„ë¥˜ ì‘ì—…ë§Œí¼ ì‰½ì§€ ì•Šë‹¤.\n",
    "- ë‹¨ìˆœíˆ ì°¸ì¡° ë¬¸ì¥ê³¼ í›„ë³´ ë¬¸ì¥ì´ ì •í™•íˆ ì¼ì¹˜í•˜ëŠ”ì§€ í™•ì¸í•˜ëŠ” ê²ƒì€ ìµœì„ ì´ ì•„ë‹ˆë‹¤.\n",
    "- ì‚¬ëŒì´ ì§ì ‘ ì‘ì„±í•œ \"Gold Standard\" ì •ë‹µë„ ì—¬ëŸ¬ê°œì¼ ìˆ˜ ìˆê¸° ë•Œë¬¸ì´ë‹¤."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9wwhhi-9jcUr"
   },
   "source": [
    "### 6.4.1 BLEU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QuDntdCOeu77"
   },
   "source": [
    "- BLEU: Bilingual Evaluation Understudy\n",
    "- Papineni, Kishore, et al. \"Bleu: a method for automatic evaluation of machine translation.\" Proceedings of the 40th annual meeting of the Association for Computational Linguistics. 2002.\n",
    "- í•œ ê°œì˜ ìƒì„±ëœ í›„ë³´ ë¬¸ì¥ $sent$ ì™€ (ë³µìˆ˜ì˜) ì°¸ì¡° ë¬¸ì¥ $sent'$ ì‚¬ì´ì—, ğ‘›-gramë“¤ì´ ì–¼ë§ˆë‚˜ ì¼ì¹˜í•˜ëŠ”ì§€ë¥¼ ë¹„êµ.\n",
    "- ë¹„êµ ë°©ë²•: ìœ ì‚¬ë„(modified ğ‘›-gram precision)\n",
    "- Modified ğ‘›-gram Precision:\n",
    "$$p_n=\\frac{\\sum_{snt \\in C}\\sum_{n\\mathrm{gram} \\in snt'}Count_{\\mathrm{clip}}(n\\mathrm{gram})}{\\sum_{snt' \\in C}\\sum_{n\\mathrm{gram} \\in snt}Count(n\\mathrm{gram})}$$\n",
    "- Brevity í˜ë„í‹° (BR)\n",
    "$$BR=\\mathrm{min}\\left(1, e^{1-l_{ref}/l_{gen}}\\right)$$\n",
    "- BLEU Score:  \n",
    "$$\\mathrm{BLEU}_N=BR \\times \\left(\\prod_{n=1}^{N}p_n\\right)^{1/N}$$\n",
    "- BLEUëŠ” ë™ì˜ì–´ë¥¼ ê³ ë ¤í•˜ì§€ ì•Šìœ¼ë©°, ìœ ë„ëœ ì‹ì˜ ë§ì€ ë‹¨ê³„ì— í—ˆìˆ í•¨ì´ ì¡´ì¬í•œë‹¤.\n",
    "- BLEUì˜ ë‹¨ì ì— ëŒ€í•œ ì°¸ê³  ë¬¸í—Œ:\n",
    "[Evaluating Text Output in NLP: BLEU at Your Own Risk](https://towardsdatascience.com/evaluating-text-output-in-nlp-bleu-at-your-own-risk-e8609665a213)\n",
    "- í…ìŠ¤íŠ¸ê°€ í† í°í™”ë˜ì–´ì•¼ í•˜ë©°, í† í°í™” ë°©ë²•ì´ ë‹¤ë¥´ë©´ BLEU scoreë„ ë‹¬ë¼ì§ˆ ìˆ˜ ìˆë‹¤.\n",
    "- `SacreBLEU`: í† í°í™” ë‹¨ê³„ë¥¼ ë‚´ì¥ì‹œí‚´ -> ë²¤ì¹˜ë§ˆí‚¹ì—ì„œ ì„ í˜¸ë˜ëŠ” ë°©ë²•"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VMMH_m-3eu77"
   },
   "outputs": [],
   "source": [
    "# ì˜ë¬¸íŒ ì›ë³¸ì˜ ì†ŒìŠ¤ ì½”ë“œ ==>  ì‹¤í–‰ë˜ë‚˜, ë‚˜ì¤‘ì— evaluate() ë•Œë¬¸ì— ì˜¤ë¥˜\n",
    "# from datasets import load_metric\n",
    "\n",
    "# bleu_metric = load_metric(\"sacrebleu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lHpEGbv6eu77"
   },
   "outputs": [],
   "source": [
    "# ë²ˆì—­íŒ ì†ŒìŠ¤ì½”ë“œ\n",
    "!pip install evaluate\n",
    "import evaluate\n",
    "bleu_metric = evaluate.load(\"sacrebleu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bFJdMAJgjcUr"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "bleu_metric.add(\n",
    "    prediction=\"the the the the the the\", reference=[\"the cat is on the mat\"])\n",
    "results = bleu_metric.compute(smooth_method=\"floor\", smooth_value=0)\n",
    "results[\"precisions\"] = [np.round(p, 2) for p in results[\"precisions\"]]\n",
    "pd.DataFrame.from_dict(results, orient=\"index\", columns=[\"Value\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sp5yiZf7eu77"
   },
   "source": [
    "- `reference`: listë¥¼ ì „ë‹¬í•˜ëŠ” ì´ìœ ëŠ” ì°¸ì¡° ë¬¸ì¥ì´ ì—¬ëŸ¬ê°œ ì¼ ìˆ˜ ìˆê¸° ë•Œë¬¸\n",
    "- `smooth_method`: zero precision ë°©ì§€ë¥¼ ìœ„í•œ smoothing ë°©ë²• ì§€ì •(\"floor\"ì´ë©´ smooth_value=0.1 ì´ default)\n",
    "- `smooth_value`: n-gramì´ í•˜ë‚˜ë„ ì—†ì„ ë•Œ precisionì´ 0 ì´ë˜ëŠ” ê²ƒì„ ë°©ì§€í•˜ê¸° ìœ„í•¨(ì—¬ê¸°ì„œëŠ” 0 ìœ¼ë¡œ í• ë‹¹í•˜ì—¬ smoothing ê¸°ëŠ¥ì„ ë”)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "69WlgjG6jcUr"
   },
   "outputs": [],
   "source": [
    "bleu_metric.add(\n",
    "    prediction=\"the cat is on mat\", reference=[\"the cat is on the mat\"])\n",
    "results = bleu_metric.compute(smooth_method=\"floor\", smooth_value=0)\n",
    "results[\"precisions\"] = [np.round(p, 2) for p in results[\"precisions\"]]\n",
    "pd.DataFrame.from_dict(results, orient=\"index\", columns=[\"Value\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i9vCtyDRjcUs"
   },
   "source": [
    "### 6.4.2 ROUGE\n",
    "- Lin, Chin-Yew. \"Rouge: A package for automatic evaluation of summaries.\" Text summarization branches out. 2004.\n",
    "- ë²ˆì—­ì—ì„œëŠ” ê°€ëŠ¥í•˜ê³  ì ì ˆí•œ ëª¨ë“  ë‹¨ì–´ë¥¼ í¬í•¨í•˜ëŠ” ë²ˆì—­ ë³´ë‹¤ëŠ” ì •í™•ë„(Precision)ê°€ ë†’ì€ ë²ˆì—­ì´ ì„ í˜¸ë˜ë¯€ë¡œ ê¸°ê³„ë²ˆì—­ì—ì„œëŠ” `BLEU`ê°€ ë§ì´ ì‚¬ìš©ëœë‹¤.\n",
    "- Summarization ê°™ì€ ì‘ì—…ì—ì„œëŠ” ìƒì„±ëœ í…ìŠ¤íŠ¸ì— ëª¨ë“  ì¤‘ìš”í•œ ì •ë³´ê°€ í¬í•¨ë˜ê¸°ë¥¼ ì›í•˜ë¯€ë¡œ, ì¬í˜„ìœ¨(Recall)ì´ ë†’ê¸°ë¥¼ ì›í•˜ë©°, ì´ëŸ° ê²½ìš° `ROUGE`ê°€ ì‚¬ìš©ëœë‹¤.\n",
    "- BLEU: í›„ë³´ ë¬¸ì¥ì˜ n-gramì´ ì°¸ì¡° ë¬¸ì¥ì— ì–¼ë§ˆë‚˜ ë§ì´ ë“±ì¥í•˜ëŠ”ì§€ í™•ì¸\n",
    "- ROUGE: ì°¸ì¡° ë¬¸ì¥ì˜ n-gramì´ í›„ë³´ ë¬¸ì¥ì— ì–¼ë§ˆë‚˜ ë§ì´ ë“±ì¥í•˜ëŠ”ì§€ í™•ì¸ <br>\n",
    "<br>\n",
    "$$\\mathrm{ROUGE}_N=\\frac{\\sum_{snt' \\in C}\\sum_{n\\mathrm{gram} \\in snt'}Count_{\\mathrm{match}}(n\\mathrm{gram})}{\\sum_{snt' \\in C}\\sum_{n\\mathrm{gram} \\in snt'}Count(n\\mathrm{gram})}$$\n",
    "<br>\n",
    "- ROUGE-L: ê°€ì¥ ê¸´ ê³µí†µ ë¶€ë¶„ ì‹œí€€ìŠ¤(LCS: Longest Common Subsequence)ë¥¼ ì¸¡ì •í•˜ëŠ” ì§€í‘œ\n",
    "- ë‘ ìƒ˜í”Œì„ ë¹„êµí•  ë•Œ ê¸´ ë¬¸ì¥ì´ ìœ ë¦¬í•˜ë¯€ë¡œ ì–´ë–¤ ì‹ìœ¼ë¡œë“  ì •ê·œí™”ê°€ í•„ìš”í•˜ë‹¤. ì´ë¥¼ ìœ„í•´ F-score ê°™ì€ ë°©ì‹ìœ¼ë¡œ $F_{LCS}$ë¥¼ ì‚¬ìš©í•œë‹¤.\n",
    "  - m: ì°¸ì¡° ë¬¸ì¥ì˜ ê¸¸ì´\n",
    "  - n: í›„ë³´ ë¬¸ì¥ì˜ ê¸¸ì´\n",
    "$$R_{LCS}=\\frac{LCS(X, Y)}{m}$$\n",
    "$$P_{LCS}=\\frac{LCS(X, Y)}{n}$$\n",
    "\n",
    "$$F_{LCS}=\\frac{{(1+\\beta^2)R_{LCS}P_{LCS}}}{R_{LCS}+\\beta^2P_{LCS}}, \\mathrm{\\qquad where}\\quad \\beta=\\frac{P_{LCS}}{R_{LCS}}$$  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LFMz8QaVeu77"
   },
   "source": [
    "ğŸ¤—Datasetsì—ì„œ ì œê³µí•˜ëŠ” ROUGE ì ìˆ˜ ì¢…ë¥˜:\n",
    "- ROUGE-L: ë¬¸ì¥ë§ˆë‹¤ ì ìˆ˜ë¥¼ ê³„ì‚°í•´ì„œ ìš”ì•½ì— ëŒ€í•´ í‰ê· í•œ ì ìˆ˜\n",
    "- ROUGE-Lsum: ì „ì²´ ìš”ì•½ì— ëŒ€í•´ ì§ì ‘ ê³„ì‚°í•œ ì ìˆ˜"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ejIkuSU8jcUs"
   },
   "outputs": [],
   "source": [
    "rouge_metric = evaluate.load(\"rouge\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "C68XR0QwjcUs"
   },
   "outputs": [],
   "source": [
    "reference = dataset[\"train\"][1][\"highlights\"]\n",
    "records = []\n",
    "rouge_names = [\"rouge1\", \"rouge2\", \"rougeL\", \"rougeLsum\"]\n",
    "\n",
    "for model_name in summaries:\n",
    "    rouge_metric.add(prediction=summaries[model_name], reference=reference)\n",
    "    score = rouge_metric.compute()\n",
    "    rouge_dict = dict((rn, score[rn]) for rn in rouge_names)\n",
    "    records.append(rouge_dict)\n",
    "pd.DataFrame.from_records(records, index=summaries.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GKWlU7I1eu77"
   },
   "source": [
    "PEGASUS ë…¼ë¬¸ì— ë”°ë¥´ë©´, CNN/DaylyMail ë°ì´í„°ì…‹ì—ì„œ T5 ë³´ë‹¤ ë›°ì–´ë‚˜ë©°, ì ì–´ë„ BARTì— ê²¬ì¤„ë§Œ í•˜ë‹¤ê³  ì£¼ì¥. PEGASUS ë…¼ë¬¸ì˜ ê²°ê³¼ê°€ ì¬í˜„ë˜ëŠ”ì§€ ì‚´í´ë³´ì."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8BeErS6qjcUs"
   },
   "source": [
    "## 6.5 CNN/DailyMail ë°ì´í„°ì…‹ì—ì„œ PEGASUS í‰ê°€í•˜ê¸°"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YKJIoeCLeu78"
   },
   "source": [
    "ë…¸íŠ¸ë¶ ì¤‘ê°„ë¶€í„° ì‹¤í–‰í•˜ëŠ” ê²½ìš°ì—ë§Œ ë‹¤ìŒ Cellì„ ì‹¤í–‰í•  ê²ƒ!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ayYWw-z4jcUs"
   },
   "outputs": [],
   "source": [
    "# ì´ ì…€ì€ ë…¸íŠ¸ë¶ ì¤‘ê°„ë¶€í„° ì‹¤í–‰í•˜ê¸° ìœ„í•œ ê²ƒì…ë‹ˆë‹¤. ì‹¤í–‰í•˜ë ¤ë©´ ì£¼ì„ì²˜ë¦¬ë¥¼ ì‚­ì œí•˜ì\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from datasets import load_dataset\n",
    "import evaluate\n",
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "\n",
    "# \"cnn_dailymail\" ë°ì´í„°ì…‹ ë‹¤ìš´ë¡œë“œ ì—ëŸ¬ê°€ ë°œìƒí•  ê²½ìš° ëŒ€ì‹  \"ccdv/cnn_dailymail\"ì„ ì‚¬ìš©í•˜ì„¸ìš”.\n",
    "dataset = load_dataset(\"ccdv/cnn_dailymail\", version=\"3.0.0\")\n",
    "rouge_metric = evaluate.load(\"rouge\", cache_dir=None)\n",
    "rouge_names = [\"rouge1\", \"rouge2\", \"rougeL\", \"rougeLsum\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zHLX0Rfueu78"
   },
   "source": [
    "- ì´ì œ ëª¨ë¸ì„ ì ì ˆíˆ í‰ê°€í•  ìš”ì†Œë¥¼ ëª¨ë‘ ê°–ì·„ë‹¤.\n",
    "- ìš°ì„  ì²˜ìŒ ì„¸ ë¬¸ì¥ì„ ì‚¬ìš©í•˜ëŠ” baseline modelì˜ ì„±ëŠ¥ë¶€í„° í‰ê°€í•˜ì."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aToVS3ZKjcUs"
   },
   "outputs": [],
   "source": [
    "def evaluate_summaries_baseline(dataset, metric,\n",
    "                                column_text=\"article\",\n",
    "                                column_summary=\"highlights\"):\n",
    "    summaries = [three_sentence_summary(text) for text in dataset[column_text]]\n",
    "    metric.add_batch(predictions=summaries,\n",
    "                     references=dataset[column_summary])\n",
    "    score = metric.compute()\n",
    "    return score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Nje8xa5xeu78"
   },
   "source": [
    "- CNN/DaylyMail ë°ì´í„°ì…‹ì˜ í…ŒìŠ¤íŠ¸ì…‹ì˜ ìƒ˜í”Œìˆ˜ = ~10,000 ê°œ\n",
    "- ìš”ì•½ ìƒì„±ì—ëŠ” ë§ì€ ì‹œê°„ì´ ê±¸ë¦°ë‹¤.\n",
    "  - ìƒì„±ë˜ëŠ” ëª¨ë“  í† í°ì´ forward passë¥¼ ê±°ì³ì•¼ í•˜ë¯€ë¡œ\n",
    "  - ìƒ˜í”Œë§ˆë‹¤ 100ê°œì˜ í† í°ì„ ìƒì„±í•˜ë ¤ë©´ 100 x 10000 = 1ë°±ë§Œë²ˆì˜ forward passë¥¼ ê±°ì³ì•¼ í•¨.\n",
    "- ì›ë˜ ì €ìì˜ ì±…ì—ëŠ” ì‹œê°„ì„ ì¤„ì´ê¸° ìœ„í•´ í…ŒìŠ¤íŠ¸ì…‹ì—ì„œ 1000ê°œì„ ìƒ˜í”Œë§í•˜ì—¬ ì‚¬ìš©.\n",
    "- ì‹¤ìŠµì—ì„œëŠ” ì‹œê°„ì„ ë” ì¤„ì´ê¸° ìœ„í•´ 200ê°œë§Œ ì‚¬ìš©í•˜ê¸°ë¡œ í•¨"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DVoNhsBfeu78"
   },
   "source": [
    "ìš°ì„  baseline ëª¨ë¸ í‰ê°€."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ycmRh13IjcUt"
   },
   "outputs": [],
   "source": [
    "test_sampled = dataset[\"test\"].shuffle(seed=42).select(range(200))  # 1000ì„ 200ìœ¼ë¡œ ìˆ˜ì • - kikim6114\n",
    "\n",
    "score = evaluate_summaries_baseline(test_sampled, rouge_metric)\n",
    "rouge_dict = dict((rn, score[rn]) for rn in rouge_names)\n",
    "pd.DataFrame.from_dict(rouge_dict, orient=\"index\", columns=[\"baseline\"]).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VhXWGR8Xeu78"
   },
   "source": [
    "PEGASUS ëª¨ë¸ í‰ê°€ë¥¼ ìœ„í•œ í•¨ìˆ˜:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uW1yflZTjcUt"
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import torch\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "def chunks(list_of_elements, batch_size):\n",
    "    \"\"\"list_of_elementsë¡œë¶€í„° batch_size í¬ê¸°ì˜ ì²­í¬ë¥¼ ì—°ì†ì ìœ¼ë¡œ ìƒì„±í•©ë‹ˆë‹¤\"\"\"\n",
    "    for i in range(0, len(list_of_elements), batch_size):\n",
    "        yield list_of_elements[i : i + batch_size]\n",
    "\n",
    "def evaluate_summaries_pegasus(dataset, metric, model, tokenizer,\n",
    "                               batch_size=16, device=device,\n",
    "                               column_text=\"article\",\n",
    "                               column_summary=\"highlights\"):\n",
    "    article_batches = list(chunks(dataset[column_text], batch_size))\n",
    "    target_batches = list(chunks(dataset[column_summary], batch_size))\n",
    "\n",
    "    for article_batch, target_batch in tqdm(\n",
    "        zip(article_batches, target_batches), total=len(article_batches)):\n",
    "\n",
    "        inputs = tokenizer(article_batch, max_length=1024,  truncation=True,\n",
    "                        padding=\"max_length\", return_tensors=\"pt\")\n",
    "\n",
    "        summaries = model.generate(input_ids=inputs[\"input_ids\"].to(device),\n",
    "                         attention_mask=inputs[\"attention_mask\"].to(device),\n",
    "                         length_penalty=0.8, num_beams=8, max_length=128)  # ë…¼ë¬¸ê³¼ ë™ì¼í•œ ë§¤ê°œë³€ìˆ˜ ê°’ ì‚¬ìš©\n",
    "\n",
    "        decoded_summaries = [tokenizer.decode(s, skip_special_tokens=True,\n",
    "                                clean_up_tokenization_spaces=True)\n",
    "               for s in summaries]\n",
    "        decoded_summaries = [d.replace(\"<n>\", \" \") for d in decoded_summaries]\n",
    "        metric.add_batch(predictions=decoded_summaries, references=target_batch)\n",
    "\n",
    "    score = metric.compute()\n",
    "    return score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jMQSvRcZeu78"
   },
   "source": [
    "ì´ì œ seq2seq ìƒì„± ì‘ì—…ì— ì‚¬ìš©ë˜ëŠ” `AutoModelForSeq2SeqLM`ì„ ì‚¬ìš©í•˜ì—¬ ëª¨ë¸ì„ ë‹¤ì‹œ ë¡œë“œí•˜ì—¬ í‰ê°€í•´ë³¸ë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kG5LwCqKjcUt"
   },
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "\n",
    "model_ckpt = \"google/pegasus-cnn_dailymail\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_ckpt)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_ckpt).to(device)\n",
    "score = evaluate_summaries_pegasus(test_sampled, rouge_metric,\n",
    "                                   model, tokenizer, batch_size=8)\n",
    "rouge_dict = dict((rn, score[rn]) for rn in rouge_names)\n",
    "pd.DataFrame(rouge_dict, index=[\"pegasus\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0H9GihK_jcUt"
   },
   "source": [
    "## 6.6 ìš”ì•½ ëª¨ë¸ í›ˆë ¨í•˜ê¸°\n",
    "- ì•ì˜ ë‚´ìš©ì„ í™œìš©í•´ Text Summarization ëª¨ë¸ì„ ì§ì ‘ í›ˆë ¨í•´ë³´ì.\n",
    "- ì‚¼ì„±ì´ ë§Œë“  SAMsum ë°ì´í„°ì…‹ì„ ì‚¬ìš©: ê³ ê°ì„¼í„° ëŒ€í™” ë‚´ìš©"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "s7cTuWU0jcUu"
   },
   "outputs": [],
   "source": [
    "dataset_samsum = load_dataset(\"samsum\")\n",
    "split_lengths = [len(dataset_samsum[split])for split in dataset_samsum]\n",
    "\n",
    "print(f\"ë¶„í•  í¬ê¸°: {split_lengths}\")\n",
    "print(f\"íŠ¹ì„±: {dataset_samsum['train'].column_names}\")\n",
    "print(\"\\nëŒ€í™”:\")\n",
    "print(dataset_samsum[\"test\"][0][\"dialogue\"])\n",
    "print(\"\\nSummary:\")\n",
    "print(dataset_samsum[\"test\"][0][\"summary\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wdwrUZ1EjcUu"
   },
   "source": [
    "### 6.6.1 SAMSumì—ì„œ PEGASUS í‰ê°€í•˜ê¸°\n",
    "- PUGASUSë¡œ ìš”ì•½ íŒŒì´í”„ë¼ì¸ì„ ì‹¤í–‰í•´ ì–´ë–»ê²Œ ì¶œë ¥ë˜ëŠ”ì§€ ë³´ì."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VXkcV0ZJjcUu"
   },
   "outputs": [],
   "source": [
    "pipe_out = pipe(dataset_samsum[\"test\"][0][\"dialogue\"])\n",
    "print(\"ìš”ì•½:\")\n",
    "print(pipe_out[0][\"summary_text\"].replace(\" .<n>\", \".\\n\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IOvphcW2eu79"
   },
   "outputs": [],
   "source": [
    "SAMsumì—ì„œì˜ ìš”ì•½ì€ CNN/DailyMailì—ì„œë³´ë‹¤ ì¶”ìƒì ì´ë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vstjloEJjcUu"
   },
   "outputs": [],
   "source": [
    "score = evaluate_summaries_pegasus(dataset_samsum[\"test\"], rouge_metric, model,\n",
    "                                   tokenizer, column_text=\"dialogue\",\n",
    "                                   column_summary=\"summary\", batch_size=8)\n",
    "\n",
    "rouge_dict = dict((rn, score[rn]) for rn in rouge_names)\n",
    "pd.DataFrame(rouge_dict, index=[\"pegasus\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XE7eApstjcUu",
    "outputId": "a752a6e6-7966-41f9-eedf-88c7db4b7c16"
   },
   "source": [
    "ê²°ê³¼ê°€ í›Œë¥­í•˜ì§€ëŠ” ì•Šì§€ë§Œ CNN/DailyMail ë°ì´í„°ì…‹ì´ SAMsum ê³¼ í¬ê²Œ ë‹¤ë¥´ê¸° ë•Œë¬¸ì— ì–´ëŠ ì •ë„ ì˜ˆìƒí•  ìˆ˜ ìˆëŠ” ê²°ê³¼ë‹¤.\n",
    "í›ˆë ¨ ì „ì— íŒŒì´í”„ë¼ì¸ì„ ì¤€ë¹„í•˜ë©´ ë‘ ê°€ì§€ ì‡ì ì´ ìˆë‹¤:\n",
    "- í›ˆë ¨ì´ ì„±ê³µì ì¸ì§€ ë°”ë¡œ í‰ê°€ê°€ ê°€ëŠ¥\n",
    "- ê¸°ì¤€ì ì´ ìˆ˜ë¦½ë¨  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ckdWOLreeu79",
    "outputId": "a752a6e6-7966-41f9-eedf-88c7db4b7c16"
   },
   "source": [
    "ì´ ë°ì´í„°ì…‹ì—ì„œ ëª¨ë¸ì„ ë¯¸ì„¸ì¡°ì •í•˜ë©´ ROUGE ì ìˆ˜ê°€ ë°”ë¡œ í–¥ìƒë˜ì–´ì•¼ í•œë‹¤. ê·¸ë ‡ì§€ ì•Šìœ¼ë©´ í›ˆë ¨ê³¼ì •ì— ë¬¸ì œê°€ ìˆëŠ” ê²ƒì´ë‹¤."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Xfqm_kNxjcUu"
   },
   "source": [
    "### 6.6.2 PEGASUS ë¯¸ì„¸ íŠœë‹í•˜ê¸°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XhgbQsMFjcUu"
   },
   "outputs": [],
   "source": [
    "d_len = [len(tokenizer.encode(s)) for s in dataset_samsum[\"train\"][\"dialogue\"]]\n",
    "s_len = [len(tokenizer.encode(s)) for s in dataset_samsum[\"train\"][\"summary\"]]\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(10, 3.5), sharey=True)\n",
    "axes[0].hist(d_len, bins=20, color=\"C0\", edgecolor=\"C0\")\n",
    "axes[0].set_title(\"Dialogue Token Length\")\n",
    "axes[0].set_xlabel(\"Length\")\n",
    "axes[0].set_ylabel(\"Count\")\n",
    "axes[1].hist(s_len, bins=20, color=\"C0\", edgecolor=\"C0\")\n",
    "axes[1].set_title(\"Summary Token Length\")\n",
    "axes[1].set_xlabel(\"Length\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UDVRa80Zeu79"
   },
   "source": [
    "- Trainerë¥¼ ìœ„í•œ data collatorë¥¼ ë§Œë“¤ì\n",
    "  - ëŒ€í™” ìµœëŒ€ ê¸¸ì´ = 1024\n",
    "  - ìš”ì•½ ìµœëŒ€ ê¸¸ì´ = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dDa2ktBWjcUu"
   },
   "outputs": [],
   "source": [
    "def convert_examples_to_features(example_batch):\n",
    "    input_encodings = tokenizer(example_batch[\"dialogue\"], max_length=1024,\n",
    "                                truncation=True)\n",
    "\n",
    "    with tokenizer.as_target_tokenizer():  # ë²ˆì—­íŒì—ì„œëŠ” context manager with~ë¥¼ ì‚¬ìš©í•˜ì§€ ì•ŠìŒ\n",
    "        target_encodings = tokenizer(text_target=example_batch[\"summary\"], max_length=128,\n",
    "                                    truncation=True)\n",
    "\n",
    "    return {\"input_ids\": input_encodings[\"input_ids\"],\n",
    "            \"attention_mask\": input_encodings[\"attention_mask\"],\n",
    "            \"labels\": target_encodings[\"input_ids\"]}\n",
    "\n",
    "dataset_samsum_pt = dataset_samsum.map(convert_examples_to_features,\n",
    "                                       batched=True)\n",
    "columns = [\"input_ids\", \"labels\", \"attention_mask\"]\n",
    "dataset_samsum_pt.set_format(type=\"torch\", columns=columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1Y-mUdIXeu7-"
   },
   "source": [
    "`with tokenizer.as_target_tokenizer()`: ì¸ì½”ë”ì™€ ë””ì½”ë”ì˜ tokenizerë¥¼ êµ¬ë³„í•˜ëŠ” ê²ƒì´ ì¤‘ìš”í•  ë•Œ ì‚¬ìš©"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oYkcJ3M-jcUv"
   },
   "outputs": [],
   "source": [
    "# í‹°ì²˜ í¬ì‹±(teacher forcing)\n",
    "# í…ìŠ¤íŠ¸ ìƒì„±ì„ ìœ„í•œ ë””ì½”ë” ì…ë ¥ê³¼ ë ˆì´ë¸”ì˜ ì •ë ¬\n",
    "text = ['PAD','Transformers', 'are', 'awesome', 'for', 'text', 'summarization']\n",
    "rows = []\n",
    "for i in range(len(text)-1):\n",
    "    rows.append({'step': i+1, 'decoder_input': text[:i+1], 'label': text[i+1]})\n",
    "pd.DataFrame(rows).set_index('step')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kxJVzh6bjcUv"
   },
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForSeq2Seq\n",
    "\n",
    "seq2seq_data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KlubqTm4jcUv"
   },
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments, Trainer\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='pegasus-samsum', num_train_epochs=1, warmup_steps=500,\n",
    "    per_device_train_batch_size=1, per_device_eval_batch_size=1,\n",
    "    weight_decay=0.01, logging_steps=10, push_to_hub=True,\n",
    "    evaluation_strategy='steps', eval_steps=500, save_steps=1e6,\n",
    "    gradient_accumulation_steps=16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NyrgBHmbeu7-"
   },
   "source": [
    "- `gradient_accumulation_steps=16`: ëª¨ë¸ì´ í¬ë©´ batch_size=1 ë¡œ í•˜ëŠ”ë°, batchê°€ ë„ˆë¬´ ì‘ìœ¼ë©´ ìˆ˜ë ´í•˜ì§€ ì•ŠëŠ”ë‹¤.\n",
    "- ì´ ë¬¸ì œë¥¼ í•´ê²°í•˜ê¸° ìœ„í•œ ê¸°ë²•ìœ¼ë¡œì„œ, 16íšŒ ê·¸ë˜ë””ì–¸íŠ¸ê°€ ëˆ„ì ë˜ì–´ ì¶©ë¶„íˆ ì»¤ì§€ë©´ ìµœì í™” ë‹¨ê³„ê°€ ì§„í–‰ë¨\n",
    "- í•™ìŠµ ì†ë„ëŠ” ëŠë ¤ì§€ì§€ë§Œ GPU ë©”ëª¨ë¦¬ê°€ ë§ì´ ì ˆì•½ë¨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "j_fJds7jjcUv"
   },
   "outputs": [],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ce7eFBgcjcUv"
   },
   "outputs": [],
   "source": [
    "trainer = Trainer(model=model, args=training_args,\n",
    "                  tokenizer=tokenizer, data_collator=seq2seq_data_collator,\n",
    "                  train_dataset=dataset_samsum_pt[\"train\"],\n",
    "                  eval_dataset=dataset_samsum_pt[\"validation\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "O92Rj1ynjcUv"
   },
   "outputs": [],
   "source": [
    "trainer.train()\n",
    "score = evaluate_summaries_pegasus(\n",
    "    dataset_samsum[\"test\"], rouge_metric, trainer.model, tokenizer,\n",
    "    batch_size=2, column_text=\"dialogue\", column_summary=\"summary\")\n",
    "\n",
    "rouge_dict = dict((rn, score[rn]) for rn in rouge_names)\n",
    "pd.DataFrame(rouge_dict, index=[f\"pegasus\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SA-zWnwTjcUw"
   },
   "outputs": [],
   "source": [
    "trainer.push_to_hub(\"Training complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3woac7eGjcUw"
   },
   "source": [
    "### 6.6.3 ëŒ€í™” ìš”ì•½ ìƒì„±í•˜ê¸°"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D7o7-6m-eu8A"
   },
   "source": [
    "Loss ê°’ê³¼ ROUGE ì ìˆ˜ë¥¼ ë³´ë©´ CNN/DailyMailì—ì„œë§Œ í›ˆë ¨í•œ ì›ë˜ ëª¨ë¸ë³´ë‹¤ í¬ê²Œ í–¥ìƒëœ ê²ƒ ê°™ë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3fRqCknWjcUw"
   },
   "outputs": [],
   "source": [
    "import transformers\n",
    "transformers.logging.set_verbosity_error()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "o4S3WJDNjcUw"
   },
   "outputs": [],
   "source": [
    "gen_kwargs = {\"length_penalty\": 0.8, \"num_beams\":8, \"max_length\": 128}\n",
    "sample_text = dataset_samsum[\"test\"][0][\"dialogue\"]\n",
    "reference = dataset_samsum[\"test\"][0][\"summary\"]\n",
    "# `haesun`ë¥¼ ìì‹ ì˜ í—ˆë¸Œ ì‚¬ìš©ì ì´ë¦„ìœ¼ë¡œ ë°”ê¾¸ì„¸ìš”.\n",
    "pipe = pipeline(\"summarization\", model=\"haesun/pegasus-samsum\")\n",
    "\n",
    "print(\"ëŒ€í™”:\")\n",
    "print(sample_text)\n",
    "print(\"\\nì°¸ì¡° ìš”ì•½:\")\n",
    "print(reference)\n",
    "print(\"\\nëª¨ë¸ ìš”ì•½:\")\n",
    "print(pipe(sample_text, **gen_kwargs)[0][\"summary_text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WbJ5OjrWeu8A"
   },
   "source": [
    "- ì°¸ì¡° ë¬¸ì¥ê³¼ í›¨ì”¬ ë” ë¹„ìŠ·í•´ì¡Œë‹¤. ëª¨ë¸ì´ ê·¸ëƒ¥ ë¬¸ì¥ì„ ì¶”ì¶œí•˜ì§€ ì•Šê³  ëŒ€í™”ë¥¼ í•©ì„±í•´ì„œ ìš”ì•½ì„ ë§Œë“œëŠ” ë²•ì„ ë°°ìš´ê²ƒ ê°™ë‹¤.\n",
    "- ì‹¤ì œ ëŒ€í™” ì…ë ¥ì—ì„œ ì´ ëª¨ë¸ì€ ì–¼ë§ˆë‚˜ ì˜ ì‘ë™í• ê¹Œ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NdavQ8oijcUw"
   },
   "outputs": [],
   "source": [
    "custom_dialogue = \"\"\"\\\n",
    "Thom: Hi guys, have you heard of transformers?\n",
    "Lewis: Yes, I used them recently!\n",
    "Leandro: Indeed, there is a great library by Hugging Face.\n",
    "Thom: I know, I helped build it ;)\n",
    "Lewis: Cool, maybe we should write a book about it. What do you think?\n",
    "Leandro: Great idea, how hard can it be?!\n",
    "Thom: I am in!\n",
    "Lewis: Awesome, let's do it together!\n",
    "\"\"\"\n",
    "print(pipe(custom_dialogue, **gen_kwargs)[0][\"summary_text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZTpcHZGujcUw"
   },
   "source": [
    "## 6.7 ê²°ë¡ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xG-9-iXXeu8A"
   },
   "source": [
    "- Text Summarizationì€ sentiment analysis, NER, QA ê°™ì€ ë¶„ë¥˜ ì‘ì—…ë“¤ë³´ë‹¤ ëª‡ê°€ì§€ íŠ¹ìˆ˜í•œ ì–´ë ¤ì›€ì´ ìˆë‹¤.\n",
    "- BLEUë‚˜ ROUGEê°€ ìˆì§€ë§Œ ì—­ì‹œ ì‚¬ëŒì˜ íŒë‹¨ì´ ê°€ì¥ ì¢‹ì€ ì²™ë„ë‹¤\n",
    "- Summarization ëª¨ë¸ë¡œ ì‘ì—…ì„ ìˆ˜í–‰í•  ë•Œ, ë¬¸ë§¥ í¬ê¸° ë³´ë‹¤ëŠ” ê¸´ í…ìŠ¤íŠ¸ë¥¼ ìš”ì•½í•˜ëŠ” ë°©ë²•ì— ì˜ë¬¸ì„ ê°–ê²Œ ëœë‹¤.\n",
    "- OpenAIëŠ” ê¸´ ë¬¸ì„œì— ë°˜ë³µì ìœ¼ë¡œ ëª¨ë¸ì„ ì ìš©í•˜ê³  ì‚¬ëŒì˜ í”¼ë“œë°±ì„ ë°˜ë³µ ë£¨í”„ì— ì¶”ê°€í•˜ì—¬ summarization taskì˜ scaleì„ í™•ì¥í–ˆë‹¤.\n",
    "- Wu, Jeff, et al. \"Recursively summarizing books with human feedback.\" arXiv preprint arXiv:2109.10862 (2021)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5cyO4rcceu8A"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuClass": "premium",
   "gpuType": "L4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "NLP",
   "language": "python",
   "name": "nlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "0088de2f78bb4a3aaf64536df94ff2af": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "114118bc4d60432eb6e5920c852c4516": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "17cdf4fcffa243d3a3038aa4f7562562": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "21218838820e479b92c8f3dbec98f7b4": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "33b2d082615143ca9c40b5b6a98e4fc8": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_d834ca2ee2ff4cc9a80382b216caaa79",
       "IPY_MODEL_ba12f39c667b4461901d191df39f4a20",
       "IPY_MODEL_9ed594484a5040f38b04d95fbc8448ea"
      ],
      "layout": "IPY_MODEL_114118bc4d60432eb6e5920c852c4516"
     }
    },
    "3f03d7b3f0ab448199f7e31986f98fe7": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "9ed594484a5040f38b04d95fbc8448ea": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_17cdf4fcffa243d3a3038aa4f7562562",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_3f03d7b3f0ab448199f7e31986f98fe7",
      "value": "â€‡15.6k/15.6kâ€‡[00:00&lt;00:00,â€‡1.61MB/s]"
     }
    },
    "aebe566de65c4feba8f808129e98aa08": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "ba12f39c667b4461901d191df39f4a20": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_0088de2f78bb4a3aaf64536df94ff2af",
      "max": 15586,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_21218838820e479b92c8f3dbec98f7b4",
      "value": 15586
     }
    },
    "bad8a6711b7b4290849616bbf5845888": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "d834ca2ee2ff4cc9a80382b216caaa79": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_aebe566de65c4feba8f808129e98aa08",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_bad8a6711b7b4290849616bbf5845888",
      "value": "Downloadingâ€‡readme:â€‡100%"
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
